{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Hussein S. Al-Olimat, hussein@knoesis.org\n",
    "\n",
    "This software is released under the GNU Affero General Public License (AGPL) v3.0 License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LNEx performance using the locations gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyHamcrest\n",
    "!pip install wordsegment\n",
    "!pip install shapely\n",
    "!pip install nltk\n",
    "!pip install elasticsearch\n",
    "!pip install elasticsearch_dsl\n",
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "from shapely.geometry import MultiPoint\n",
    "import operator\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"LNEx\")\n",
    "import LNEx as lnex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_they_overlap(tub1, tub2):\n",
    "    '''Checks whether two substrings of the tweet overlaps based on their start\n",
    "    and end offsets.'''\n",
    "\n",
    "    if tub2[1] >= tub1[0] and tub1[1] >= tub2[0]:\n",
    "        return True\n",
    "\n",
    "def read_annotations(filename):\n",
    "\n",
    "    filename = os.path.join(\"_Data/Brat_Annotations\", filename)\n",
    "\n",
    "    # read tweets from file to list\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "def init_using_elasticindex(bb, cache, augmentType, dataset, capital_word_shape):\n",
    "    lnex.elasticindex(conn_string='localhost:9200', index_name=\"photon\")\n",
    "\n",
    "    geo_info = lnex.initialize( bb, augmentType=augmentType,\n",
    "                                    cache=cache,\n",
    "                                    dataset_name=dataset,\n",
    "                                    capital_word_shape=capital_word_shape)\n",
    "    return geo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LNEx ...\n",
      "Done Initialization ...\n"
     ]
    }
   ],
   "source": [
    "bbs = { \"chennai\": [12.74, 80.066986084, 13.2823848224, 80.3464508057],\n",
    "        \"louisiana\": [29.4563, -93.3453, 31.4521, -89.5276],\n",
    "        \"houston\": [29.4778611958, -95.975189209, 30.1463147381, -94.8889160156],\n",
    "        \"columbus\": [39.808631, -83.2102799, 40.1572719, -82.7713781],\n",
    "        \"test\": [41.6187434973, -83.7106928844, 41.6245055116, -83.7017216664]}\n",
    "\n",
    "dataset = \"houston\"\n",
    "\n",
    "geo_info = init_using_elasticindex(bbs[dataset], cache=False, augmentType=\"HP\", \n",
    "                                   dataset=dataset, capital_word_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"101018-Brat-Annotations/\"+dataset.title()+\"_annotations.json\"\n",
    "filename = dataset.title()+\"_annotations.json\"\n",
    "filename = dataset+\"_annotations.json\"\n",
    "anns = read_annotations(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 1200 people i harris county have had to be rescued so far today due to houston flood houston via\n",
      "[('1200', 0, 3), ('people', 5, 10), ('i', 12, 12), ('harris', 14, 19), ('county', 21, 26), ('have', 28, 31), ('had', 33, 35), ('to', 37, 38), ('be', 40, 41), ('rescued', 43, 49), ('so', 51, 52), ('far', 54, 56), ('today', 58, 62), ('due', 64, 66), ('to', 68, 69), ('houston', 72, 78), ('flood', 79, 83), ('houston', 86, 92), ('via', 118, 120)]\n",
      ">>>> video : dozens of horses rescued from flood waters via houston flood\n",
      "[('video', 0, 4), (':', 5, 5), ('dozens', 7, 12), ('of', 14, 15), ('horses', 17, 22), ('rescued', 24, 30), ('from', 32, 35), ('flood', 37, 41), ('waters', 43, 48), ('via', 74, 76), ('houston', 84, 90), ('flood', 100, 104)]\n",
      ">>>> man found dead in submerged 18 wheeler in north houston via houston flood\n",
      "[('man', 0, 2), ('found', 4, 8), ('dead', 10, 13), ('in', 15, 16), ('submerged', 18, 26), ('18', 28, 29), ('wheeler', 31, 37), ('in', 39, 40), ('north', 42, 46), ('houston', 48, 54), ('via', 80, 82), ('houston', 90, 96), ('flood', 106, 110)]\n",
      ">>>> stay safe and dry houston ! ktrk news live streaming video via flood\n",
      "[('stay', 0, 3), ('safe', 5, 8), ('and', 10, 12), ('dry', 14, 16), ('houston', 19, 25), ('!', 26, 26), ('ktrk', 28, 31), ('news', 33, 36), ('live', 38, 41), ('streaming', 43, 51), ('video', 53, 57), ('via', 83, 85), ('flood', 102, 106)]\n",
      "houston\t0.8964659034345446\t0.6650664697193501\t0.763620945516218\n"
     ]
    }
   ],
   "source": [
    "TPs_count = 0\n",
    "FPs_count = 0\n",
    "FNs_count = 0\n",
    "overlaps_count = 0\n",
    "\n",
    "fns = defaultdict(int)\n",
    "\n",
    "count = 0\n",
    "one_geolocation = 0\n",
    "all_geolocation = 0\n",
    "geo_codes_length_dist = defaultdict(int)\n",
    "\n",
    "FPs_set = defaultdict(set)\n",
    "FNs_set = defaultdict(set)\n",
    "\n",
    "for key in list(anns.keys()):\n",
    "    \n",
    "    #print(key)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "    # skip the development set\n",
    "    if dataset != \"houston\" and count < 500:\n",
    "        continue\n",
    "\n",
    "    tweet_lns = set()\n",
    "    lnex_lns = set()\n",
    "    tweet_text = \"\"\n",
    "\n",
    "    for ann in anns[key]:\n",
    "        if ann != \"text\":\n",
    "            ln = anns[key][ann]\n",
    "\n",
    "            tweet_lns.add(((int(ln['start_idx']), int(ln['end_idx'])),\n",
    "                                ln['type']))\n",
    "        else:\n",
    "            tweet_text = anns[key][ann]\n",
    "            #print tweet_text\n",
    "\n",
    "            r = lnex.extract(tweet_text)\n",
    "\n",
    "            # how many are already disambiguated +++++++++++++++++++++++\n",
    "            for res in r:\n",
    "                if len(res[3]) < 2:\n",
    "                    one_geolocation += 1\n",
    "\n",
    "                    #if len(res[3]) == 0:\n",
    "                        #print res[2]\n",
    "                else:\n",
    "                    geo_codes_length_dist[len(res[3])] += 1\n",
    "\n",
    "                all_geolocation += 1\n",
    "            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            lnex_lns = set([x[1] for x in r])\n",
    "\n",
    "    #print(\"BEFORE > \", )\n",
    "    #print(\"TPs_count:[\", TPs_count, \"] FPs_count:[\", FPs_count, \"] FNs_count:[\", FNs_count, \"] overlaps_count:[\", overlaps_count, \"]\")\n",
    "    #print([(x[0], tweet_text[x[0][0]:x[0][1]]) for x in tweet_lns if x[1] == \"inLoc\"])\n",
    "    #print(lnex_lns, [tweet_text[x[0]:x[1]] for x in lnex_lns])\n",
    "    \n",
    "    tweet_lns = set([x[0] for x in tweet_lns if x[1] == \"inLoc\"])\n",
    "    \n",
    "    # True Positives +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    TPs = tweet_lns.intersection(lnex_lns)\n",
    "\n",
    "    TPs_count += len(TPs)\n",
    "\n",
    "    # Left in both sets ++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    tweet_lns -= TPs\n",
    "    lnex_lns -= TPs\n",
    "\n",
    "    # Find Overlapping LNs to be counted as 1/2 FPs and 1/2 FNs++\n",
    "    overlaps = set()\n",
    "    for x in tweet_lns:\n",
    "        for y in lnex_lns:\n",
    "            if do_they_overlap(x, y):\n",
    "                overlaps.add(x)\n",
    "                overlaps.add(y)\n",
    "\n",
    "    overlaps_count += len(overlaps)\n",
    "\n",
    "    # remove the overlapping lns from lnex_lns and tweet_lns\n",
    "    lnex_lns -= overlaps\n",
    "    tweet_lns -= overlaps\n",
    "\n",
    "    # False Positives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # lnex_lns = all - (TPs and overlaps and !inLoc)\n",
    "    FPs = lnex_lns - tweet_lns\n",
    "    FPs_count += len(FPs)\n",
    "    \n",
    "    if len(FPs) > 0:\n",
    "        #FPs_set.update(set([tweet_text[x[0]:x[1]] for x in FPs]))\n",
    "        \n",
    "        for x in FPs:\n",
    "            FPs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "        #print(\"FPs >>>>>\", key, tweet_text, [tweet_text[x[0]:x[1]] for x in FPs])\n",
    "\n",
    "    # False Negatives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    FNs = tweet_lns - lnex_lns\n",
    "    FNs_count += len(FNs)\n",
    "\n",
    "    '''if len(FNs) > 0:\n",
    "        for x in [tweet_text[x[0]:x[1]] for x in FNs]:\n",
    "            fns[x.lower()] += 1'''\n",
    "                                               \n",
    "    if len(FNs) > 0:\n",
    "        for x in FNs:\n",
    "            FNs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "\n",
    "    \n",
    "    #if len(FNs) > 0:\n",
    "    #    print(FNs, tweet_text.find(\"#houstonflood\"), [tweet_text[x[0]:x[1]] for x in lnex_lns])\n",
    "    #    print(\"FNs >>>>>\", key, tweet_text, [tweet_text[x[0]:x[1]] for x in FNs])\n",
    "\n",
    "    ####################################################################\n",
    "    #print TPs_count, FPs_count, FNs_count, overlaps_count\n",
    "    #print \"#\"*100\n",
    "    \n",
    "    #print(\"AFTER > \", )\n",
    "    #print(\"TPs_count:[\", TPs_count, \"] FPs_count:[\", FPs_count, \"] FNs_count:[\", FNs_count,\"] overlaps_count:[\", overlaps_count, \"]\")\n",
    "    #print(\"-\"*50)\n",
    "\n",
    "'''\n",
    "since we add 2 lns one from lnex_lns and one from tweet_lns if they\n",
    "overlap the equation of counting those as 1/2 FPs and 1/2 FNs is going\n",
    "to be:\n",
    "    overlaps_count x\n",
    "        1/2 (since we count twice) x\n",
    "            1/2 (since we want 1/2 of all the errors made)\n",
    "'''\n",
    "\n",
    "Precision = TPs_count/(TPs_count + FPs_count + 0.5 * .5 * overlaps_count)\n",
    "Recall = TPs_count/(TPs_count + FNs_count + 0.5 * .5 * overlaps_count)\n",
    "F_Score = (2 * Precision * Recall)/(Precision + Recall)\n",
    "\n",
    "percentage_disambiguated = one_geolocation/all_geolocation\n",
    "\n",
    "#percentage_amb_out_extracted = out_and_amb_extracted_lns/all_geolocation\n",
    "\n",
    "print (\"\\t\".join([dataset, str(Precision), str(Recall), str(F_Score)]))\n",
    "\n",
    "#for x in FPs_set:\n",
    "#    print (x, len(FPs_set[x]))\n",
    "                                               \n",
    "#for x in FNs_set:\n",
    "#    print (x, len(FNs_set[x]))                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('722190824599990300', 'f Stuebner Airline a', (54, 70))}, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lname = \"Stuebner Airline\"\n",
    "FNs_set[lname], len(FNs_set[lname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
