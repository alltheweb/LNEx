{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Hussein S. Al-Olimat, hussein@knoesis.org\n",
    "\n",
    "This software is released under the GNU Affero General Public License (AGPL) v3.0 License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LNEx performance using the locations gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyHamcrest\n",
    "!pip install wordsegment\n",
    "!pip install shapely\n",
    "!pip install nltk\n",
    "!pip install elasticsearch\n",
    "!pip install elasticsearch_dsl\n",
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "from shapely.geometry import MultiPoint\n",
    "import operator\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"LNEx\")\n",
    "import LNEx as lnex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_they_overlap(tub1, tub2):\n",
    "    '''Checks whether two substrings of the tweet overlaps based on their start\n",
    "    and end offsets.'''\n",
    "\n",
    "    if tub2[1] >= tub1[0] and tub1[1] >= tub2[0]:\n",
    "        return True\n",
    "\n",
    "def read_annotations(filename):\n",
    "\n",
    "    filename = os.path.join(\"_Data/Brat_Annotations\", filename)\n",
    "\n",
    "    # read tweets from file to list\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "def init_using_elasticindex(bb, cache, augmentType, dataset, capital_word_shape):\n",
    "    lnex.elasticindex(conn_string='localhost:9200', index_name=\"photon\")\n",
    "\n",
    "    geo_info = lnex.initialize( bb, augmentType=augmentType,\n",
    "                                    cache=cache,\n",
    "                                    dataset_name=dataset,\n",
    "                                    capital_word_shape=capital_word_shape)\n",
    "    return geo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LNEx ...\n",
      "Done Initialization ...\n"
     ]
    }
   ],
   "source": [
    "bbs = { \"chennai\": [12.74, 80.066986084, 13.2823848224, 80.3464508057],\n",
    "        \"louisiana\": [29.4563, -93.3453, 31.4521, -89.5276],\n",
    "        \"houston\": [29.4778611958, -95.975189209, 30.1463147381, -94.8889160156],\n",
    "        \"columbus\": [39.808631, -83.2102799, 40.1572719, -82.7713781],\n",
    "        \"test\": [41.6187434973, -83.7106928844, 41.6245055116, -83.7017216664]}\n",
    "\n",
    "dataset = \"louisiana\"\n",
    "\n",
    "geo_info = init_using_elasticindex(bbs[dataset], cache=False, augmentType=\"HP\", \n",
    "                                   dataset=dataset, capital_word_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"101018-Brat-Annotations/\"+dataset.title()+\"_annotations.json\"\n",
    "filename = dataset.title()+\"_annotations.json\"\n",
    "filename = dataset+\"_annotations.json\"\n",
    "anns = read_annotations(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louisiana\t0.8389180460234155\t0.8095052590572653\t0.823949246629659\n",
      "country 1\n",
      "I-55 2\n",
      "la 72\n",
      "LA 35\n",
      "BR 5\n",
      "Usa 1\n",
      "Louisiana 17\n",
      "La 22\n",
      "shreveport 1\n",
      "LSU 4\n",
      "Dubberly 1\n",
      "Bossier City 1\n",
      "Haughton 1\n",
      "Avoyelles 1\n",
      "Irene 1\n",
      "UnitedStates 1\n",
      "USA 4\n",
      "US 9\n",
      "Celtic Studios 2\n",
      "Louisian 1\n",
      "Lousiana 2\n",
      "America 1\n",
      "Byrd High 1\n",
      "Louisiana   1\n",
      "EBRSO 1\n",
      "br 1\n",
      "WDSU 1\n",
      "bentley 1\n",
      "Evangeline Area 1\n",
      "Beauregard Sheriff's Office 1\n",
      "EBR 1\n",
      "Robert 1\n",
      "Highland 2\n",
      "Rich & John's 1\n",
      "U.S 2\n",
      "Palmetto 1\n",
      "Central 1\n",
      "westward 1\n",
      "Fairgrounds 1\n",
      "Louisana 1\n",
      "Howlin' Wolf 1\n",
      "West Monroe 1\n",
      "Chatham 1\n",
      "Ruston Nursing and Rehabilitation Center LLC 1\n",
      "Loui 1\n",
      "Uber 1\n",
      "Village Life Center 1\n",
      "Northshore 1\n",
      "Second Harvest Food Bank 1\n",
      "Monsanto plant 1\n",
      "A 1\n",
      "Magnolia Bridge 1\n",
      "Titletown Brewing 1\n",
      "St. Helena 3\n",
      "Jackson 1\n",
      "central 1\n",
      "St. Amant 2\n",
      "louisiana 1\n",
      "Gulf Coast 1\n",
      "Lafayette Parish 1\n",
      "Louis 1\n",
      "St. Helena Hawk 1\n",
      "U.S. 1\n",
      "BattonRouge 1\n",
      "United States 1\n",
      "United Way 1\n",
      "LBA 1\n"
     ]
    }
   ],
   "source": [
    "TPs_count = 0\n",
    "FPs_count = 0\n",
    "FNs_count = 0\n",
    "overlaps_count = 0\n",
    "\n",
    "fns = defaultdict(int)\n",
    "\n",
    "count = 0\n",
    "one_geolocation = 0\n",
    "all_geolocation = 0\n",
    "geo_codes_length_dist = defaultdict(int)\n",
    "\n",
    "FPs_set = defaultdict(set)\n",
    "FNs_set = defaultdict(set)\n",
    "\n",
    "for key in list(anns.keys()):\n",
    "    \n",
    "    #print(key)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "    # skip the development set\n",
    "    if dataset != \"houston\" and count < 500:\n",
    "        continue\n",
    "\n",
    "    tweet_lns = set()\n",
    "    lnex_lns = set()\n",
    "    tweet_text = \"\"\n",
    "\n",
    "    for ann in anns[key]:\n",
    "        if ann != \"text\":\n",
    "            ln = anns[key][ann]\n",
    "\n",
    "            tweet_lns.add(((int(ln['start_idx']), int(ln['end_idx'])),\n",
    "                                ln['type']))\n",
    "        else:\n",
    "            tweet_text = anns[key][ann]\n",
    "            #print tweet_text\n",
    "\n",
    "            r = lnex.extract(tweet_text)\n",
    "\n",
    "            # how many are already disambiguated +++++++++++++++++++++++\n",
    "            for res in r:\n",
    "                if len(res[3]) < 2:\n",
    "                    one_geolocation += 1\n",
    "\n",
    "                    #if len(res[3]) == 0:\n",
    "                        #print res[2]\n",
    "                else:\n",
    "                    geo_codes_length_dist[len(res[3])] += 1\n",
    "\n",
    "                all_geolocation += 1\n",
    "            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            lnex_lns = set([x[1] for x in r])\n",
    "\n",
    "    #print(\"BEFORE > \", )\n",
    "    #print(\"TPs_count:[\", TPs_count, \"] FPs_count:[\", FPs_count, \"] FNs_count:[\", FNs_count, \"] overlaps_count:[\", overlaps_count, \"]\")\n",
    "    #print([(x[0], tweet_text[x[0][0]:x[0][1]]) for x in tweet_lns if x[1] == \"inLoc\"])\n",
    "    #print(lnex_lns, [tweet_text[x[0]:x[1]] for x in lnex_lns])\n",
    "    \n",
    "    tweet_lns = set([x[0] for x in tweet_lns if x[1] == \"inLoc\"])\n",
    "    \n",
    "    # True Positives +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    TPs = tweet_lns.intersection(lnex_lns)\n",
    "\n",
    "    TPs_count += len(TPs)\n",
    "\n",
    "    # Left in both sets ++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    tweet_lns -= TPs\n",
    "    lnex_lns -= TPs\n",
    "\n",
    "    # Find Overlapping LNs to be counted as 1/2 FPs and 1/2 FNs++\n",
    "    overlaps = set()\n",
    "    for x in tweet_lns:\n",
    "        for y in lnex_lns:\n",
    "            if do_they_overlap(x, y):\n",
    "                overlaps.add(x)\n",
    "                overlaps.add(y)\n",
    "\n",
    "    overlaps_count += len(overlaps)\n",
    "\n",
    "    # remove the overlapping lns from lnex_lns and tweet_lns\n",
    "    lnex_lns -= overlaps\n",
    "    tweet_lns -= overlaps\n",
    "\n",
    "    # False Positives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # lnex_lns = all - (TPs and overlaps and !inLoc)\n",
    "    FPs = lnex_lns - tweet_lns\n",
    "    FPs_count += len(FPs)\n",
    "    \n",
    "    if len(FPs) > 0:\n",
    "        #FPs_set.update(set([tweet_text[x[0]:x[1]] for x in FPs]))\n",
    "        \n",
    "        for x in FPs:\n",
    "            FPs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "#         print(\"FPs >>>>>\", key, tweet_text, [tweet_text[x[0]:x[1]] for x in FPs])\n",
    "\n",
    "    # False Negatives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    FNs = tweet_lns - lnex_lns\n",
    "    FNs_count += len(FNs)\n",
    "\n",
    "    '''if len(FNs) > 0:\n",
    "        for x in [tweet_text[x[0]:x[1]] for x in FNs]:\n",
    "            fns[x.lower()] += 1'''\n",
    "                                               \n",
    "    if len(FNs) > 0:\n",
    "        for x in FNs:\n",
    "            FNs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "\n",
    "    \n",
    "    #if len(FNs) > 0:\n",
    "    #    print(FNs, tweet_text.find(\"#houstonflood\"), [tweet_text[x[0]:x[1]] for x in lnex_lns])\n",
    "    #    print(\"FNs >>>>>\", key, tweet_text, [tweet_text[x[0]:x[1]] for x in FNs])\n",
    "\n",
    "    ####################################################################\n",
    "    #print TPs_count, FPs_count, FNs_count, overlaps_count\n",
    "    #print \"#\"*100\n",
    "    \n",
    "    #print(\"AFTER > \", )\n",
    "    #print(\"TPs_count:[\", TPs_count, \"] FPs_count:[\", FPs_count, \"] FNs_count:[\", FNs_count,\"] overlaps_count:[\", overlaps_count, \"]\")\n",
    "    #print(\"-\"*50)\n",
    "\n",
    "'''\n",
    "since we add 2 lns one from lnex_lns and one from tweet_lns if they\n",
    "overlap the equation of counting those as 1/2 FPs and 1/2 FNs is going\n",
    "to be:\n",
    "    overlaps_count x\n",
    "        1/2 (since we count twice) x\n",
    "            1/2 (since we want 1/2 of all the errors made)\n",
    "'''\n",
    "\n",
    "Precision = TPs_count/(TPs_count + FPs_count + 0.5 * .5 * overlaps_count)\n",
    "Recall = TPs_count/(TPs_count + FNs_count + 0.5 * .5 * overlaps_count)\n",
    "F_Score = (2 * Precision * Recall)/(Precision + Recall)\n",
    "\n",
    "percentage_disambiguated = one_geolocation/all_geolocation\n",
    "\n",
    "#percentage_amb_out_extracted = out_and_amb_extracted_lns/all_geolocation\n",
    "\n",
    "print (\"\\t\".join([dataset, str(Precision), str(Recall), str(F_Score)]))\n",
    "\n",
    "# for x in FPs_set:\n",
    "#    print (x, len(FPs_set[x]))\n",
    "                                               \n",
    "for x in FNs_set:\n",
    "   print (x, len(FNs_set[x]))                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('766596218873151488', ' #Jackson  ', (69, 76))}, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lname = \"Jackson\"\n",
    "# FPs_set[lname], len(FPs_set[lname])\n",
    "FNs_set[lname], len(FNs_set[lname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
